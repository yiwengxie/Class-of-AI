## PJ2_1 HMM实现NER任务

### 总体思路：

将实体标签看成隐藏状态，文字看成观测结果。通过train.txt的例子，估计出隐式马尔可夫的三个参数：初始概率矩阵、发射概率矩阵、状态转移概率矩阵。得到隐式马尔可夫模型后，通过Viterbi算法进行解码，通过观测序列得出隐藏状态序列。

### 代码实现：

代码定义了两个类`preprocess`和`HMM`，分别用来数据预处理和实现隐式马尔可夫模型。

#### 类`preprocess`如下：

​	`__init__`初始构造函数传入了：`train_file` 、`test_file` 和  `language`三个参数以便针对不同的语言的数据集读取需求。

​	`read`读取函数完成了数据的读取和预处理，打开文件夹进行读取，并将文件中的文字和标签分别放入`words_train`和`labels_train`两个二维列表中，该列表中的每一个列表代表在数据集中用空行（`\n`）隔开的一个完整的句子。该函数将根据不同的语言`Chinese` | `English`来选择不同的状态`states`，将提取出现过的所有单词作为`observations`。测试数据类似，不过只读取单词（观测序列）即可。

​	`write`为写入函数将处理完成的数据写入文件，方便`check`函数进行准确率计算。

```python
class preprocess:
    def __init__(self, train_file, test_file, language):
        self.train_file = train_file
        self.test_file = test_file
        self.language = language

    def read(self):
        # 读取训练数据
        with open(self.train_file, 'r', encoding='utf-8') as f:
            train_data = f.readlines()
        # 处理训练数据
        words_train, labels_train = [[]], [[]]
        words = []
        for line in train_data:
            if line != '\n':
                word, label = line.strip().split()
                words_train[-1].append(word)
                labels_train[-1].append(label)
                words.append(word)
            else:
                words_train.append([])
                labels_train.append([])
        while [] in words_train:
            words_train.remove([])
        while [] in labels_train:
            labels_train.remove([])

        if self.language.lower() == 'chinese':
            # states = sorted(list(set(Y_train)))
            # 如上方法获得的state只有28种，但是实际上有33种
            states = ['B-NAME', 'M-NAME', 'E-NAME', 'S-NAME',
                    'B-CONT', 'M-CONT', 'E-CONT', 'S-CONT',
                    'B-EDU', 'M-EDU', 'E-EDU', 'S-EDU',
                    'B-TITLE', 'M-TITLE', 'E-TITLE', 'S-TITLE',
                    'B-ORG', 'M-ORG', 'E-ORG', 'S-ORG',
                    'B-RACE', 'M-RACE', 'E-RACE', 'S-RACE',
                    'B-PRO', 'M-PRO', 'E-PRO', 'S-PRO',
                    'B-LOC', 'M-LOC', 'E-LOC', 'S-LOC', 
                    'O']
        elif self.language.lower() == 'english':
            states = ['B-PER', 'I-PER',
                    'B-ORG', 'I-ORG', 
                    'B-LOC', 'I-LOC', 
                    'B-MISC', 'I-MISC',
                    'O']
        else:
            print('language must be chinese or english')
            return
        observations = sorted(list(set(words)))
        # 读取测试数据
        with open(self.test_file, 'r', encoding='utf-8') as f:
            test_data = f.readlines()
        # 处理测试数据
        words_test = [[]]
        for line in test_data:
            if(line != '\n'):
                word, label = line.strip().split()
                words_test[-1].append(word)
            else:
                words_test.append([])
        while [] in words_test:
            words_test.remove([])

        return words_train, labels_train, words_test, states, observations
    
    def write(self, words_test, labels_pred):
        # 将预测结果写入文件
        with open('output_'+ self.language +'_for_PJ2_1.txt', 'w', encoding='utf-8') as f:
            for i in range(len(words_test)):
                for j in range(len(words_test[i])):
                    f.write(words_test[i][j] + ' ' + labels_pred[i][j] + '\n')
                f.write('\n')
```

#### 类`HMM`如下：

​	`__init__`初始函数传入了已经读取到的`states`（状态）和`observations`（观测），初始化：初始概率矩阵、转移概率矩阵、发射概率矩阵、未知单词发射概率。其中未知单词发射概率为了方便处理在测试集中出现但是没有在训练集中出现的观测。

​	`train`训练函数根据训练集得出各个矩阵。首先遍历每句话的标签即状态，对第一个状态的出现次数+1，遍历后续状态，以出现次数更新状态转移矩阵。再遍历每句话的单词即观测，对训练集句子中每一个对应状态到相应观测的次数+1。最后为了解决某些状态或观测在给定数据集中没有出现过导致其概率估计为0的情况，采用了平滑操作。同时为了使得概率总和为1，采用了归一化。这里采用了加法平滑，选择k = 1。

​	`decode`解码函数为Viterbi解码函数，使用了动态规划思想，通过计算每个时刻每个状态的最大概率和最优路径，找到最可能的状态序列。

对于每个句子`delta` = 状态数 × 单词数（观测数）， 记录每个时刻的观测下每个状态的最大概率。`phi` = 状态数 × 单词数（观测数），记录每个时刻的观测下每个状态对应的前一状态，以便获得最优路径。 

因为有未知单词的出现，所有发射概率的计算需要分类讨论。

对于每句话，首先计算初始状态概率 * 第一个观测的发射概率，得到第一个观测下不同状态的概率，再遍历后续观测，循环中遍历每个状态，计算每个状态到该观测的概率。

这个概率的计算来自于前一时刻的状态转移到该状态并获得该观测的所有概率中的最大值。当观测遍历完后，选择最后一个观测最大的状态概率，逆向获得前面的状态序列。

```python
class HMM:
    def __init__(self, states, observations):
        # states: 状态集合
        # observations: 观测集合
        # initial_prob: 初始概率
        # transition_prob: 转移概率
        # emission_prob: 发射概率
        # unknown_word_emission_prob: 未知单词的发射概率
        self.states = states
        self.observations = observations
        self.initial_prob = np.zeros(len(states))
        self.transition_prob = np.zeros((len(states), len(states)))
        self.emission_prob = np.zeros((len(states), len(observations)))
        self.unknown_word_emission_prob = np.zeros(len(states))
    
    def train(self, words, labels):
        k = 1
        # 计算初始概率、转移概率、发射概率
        for label in labels:
            self.initial_prob[self.states.index(label[0])] += 1
            for i in range(1, len(label)):
                curr_state = self.states.index(label[i])
                prev_state = self.states.index(label[i-1])
                self.transition_prob[prev_state][curr_state] += 1
        for word, label in zip(words, labels):
            for i in range(len(word)):
                state = self.states.index(label[i])
                obs = self.observations.index(word[i])
                self.emission_prob[state][obs] += 1
        # 平滑 + 归一化
        self.initial_prob = (self.initial_prob + k) / (np.sum(self.initial_prob) + k * len(self.states))
        self.transition_prob = (self.transition_prob + k) / (np.sum(self.transition_prob, axis=1, keepdims=True) + k * len(self.states))
        self.emission_prob = (self.emission_prob + k) / (np.sum(self.emission_prob, axis=1, keepdims=True) + k * (len(self.observations) + 1))
        self.unknown_word_emission_prob = np.ones(len(self.states)) / (np.sum(self.emission_prob, axis=1) + 1)  # 未知单词的发射概率等于均匀分布
    
    
    def decode(self, words):
        result = []
        # 计算delta和phi
        # delta: 每个时刻每个状态的最大概率
        # phi: 每个时刻每个状态的最大概率对应的前一个状态
        # path: 最优路径
        for word in words:
            delta = np.zeros((len(self.states), len(word)))
            phi = np.zeros((len(self.states), len(word)), dtype=int)
            if word[0] not in self.observations:
                delta[:, 0] = self.initial_prob
            else:
                obs = self.observations.index(word[0])
                delta[:, 0] = self.initial_prob * self.emission_prob[:, obs]
            for t in range(1, len(word)):
                for s in range(len(self.states)):
                    if word[t] not in self.observations:
                        p = self.transition_prob[:, s] * delta[:, t-1] * self.unknown_word_emission_prob
                    else:
                        obs = self.observations.index(word[t])
                        p = self.transition_prob[:, s] * delta[:, t-1] * self.emission_prob[s, obs]
                    delta[s, t] = np.max(p)
                    phi[s, t] = np.argmax(p)
            path = [np.argmax(delta[:, -1])]
            for t in range(len(word)-1, 0, -1):
                path.append(phi[path[-1], t])
            path.reverse()
            result.append([self.states[i] for i in path])
        return result
```

#### main函数：

读取命令行输入的操作

读取数据并作预处理

调用模型进行训练

将训练完的模型用于预测

将预测结果写入文件

```python
def main():
    # 检查命令行参数
    if len(sys.argv) != 4:
        print('Usage: python PJ2_1.py <train_file> <test_file> <language>')
        return 
    # 读取命令行参数
    train_file = sys.argv[1]
    test_file = sys.argv[2]
    language = sys.argv[3]
    # 读取数据
    pre = preprocess(train_file, test_file, language)
    words_train, labels_train, words_test, states, observations = pre.read()
    # 训练HMM模型
    hmm = HMM(states, observations)
    print('training...')
    hmm.train(words_train, labels_train)
    print('training finished')
    # 预测测试数据标签
    print('predicting...')
    labels_pred = hmm.decode(words_test)
    print('predicting finished')
    # 写入文件
    pre.write(words_test, labels_pred)
```

