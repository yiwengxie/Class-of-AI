### PJ1 第一部分 反向传播算法

#### 任务1: 拟合函数： $y = sin(x), x\in[-\pi, \pi]$

通过神经网络来拟合函数，需要一下几个部分：网络结构，随机生成数据，回归拟合，计算误差，可视化。

首先，实现网络结构，这里实现了一个可设置隐藏层数量、可设置神经元个数、可定义学习率的**bp神经网络**。具体代码如下：

`__init__`初始化构造函数要求输入神经网络的`layers`来定义**网络层数**和**神经元个数**。本次实验采用的是`[1, 10, 1]`的网络架构，三层网络，隐藏层神经元个数为10个。该函数按照输入初始化了网络架构，随机初始化了网络参数`weights`和`biases`。

`sigmod `和 `dsigmod` 函数分别为激活函数和激活函数的导数。但是在本次拟合中，并没有进行数据归一化，**结果具有负数**，而`sigmod`函数将值从实数映射到$(0,1)$区间，所以不符合本次实验要求。为了简便，本次实验采用了`tanh`作为激活函数。

`tanh` 和 `dtanh` 为激活函数和激活函数的导数。`tanh`将值从实数映射到$(-1, 1)$区间。

`forward`为前向计算，输入`input`通过网络获得`output`。每层为全连接层+激活函数。

`backward`为后向计算，在进行一次前向计算并将结果存储下来后，通过**链式法则**从后向前计算梯度，并根据学习率进行对应参数的更新。这里的损失函数采用了$loss = \frac12(\hat{y}-y)^2$，这里直接将导数带入`delta`中计算了，并没有显式地展现出来。

`train`为训练，总共训练`epochs`次，学习率为`learning_rate`，为了输入数据的多样性，训练前统一进行`reshape`操作，这里变成了列向量。

```python
class Network:
    def __init__(self, layers):
        self.layers = layers
        self.weights = []
        self.biases = []
        for i in range(1, len(layers)):
            self.weights.append(np.random.randn(layers[i], layers[i - 1]))
            self.biases.append(np.random.randn(layers[i], 1))

    def sigmod(self, x): # 掉坑里了，没有负数
        return 1 / (1 + np.exp(-x))

    def dsigmod(self, x):
        return self.sigmod(x) * (1 - self.sigmod(x))

    def tanh(self, x):
        return np.tanh(x)

    def dtanh(self, x):
        return 1 - np.square(np.tanh(x))

    def forward(self, A):
        for w, b in zip(self.weights, self.biases):
            Z = np.dot(w, A) + b
            A = self.tanh(Z)
        return A

    def backward(self, x, y, learning_rate):
        a_s = [x]
        z_s = []
        for i in range(len(self.layers) - 1):
            z = np.dot(self.weights[i], a_s[-1]) + self.biases[i]
            z_s.append(z)
            a = self.tanh(z)
            a_s.append(a)
        delta = (a_s[-1] - y) * self.dtanh(z_s[-1])
        for i in range(len(self.layers) - 2, -1, -1):
            dw = np.dot(delta, a_s[i].T)
            db = delta
            if i != 0:
                delta = np.dot(self.weights[i].T, delta) * self.dtanh(z_s[i - 1])
            self.weights[i] -= learning_rate * dw
            self.biases[i] -= learning_rate * db

    def train(self, X, Y, epochs, learning_rate):
        for epochs in range(epochs):
            for i in range(len(X)):
                x = X[i].reshape(-1, 1)
                y = Y[i].reshape(-1, 1)
                self.backward(x, y, learning_rate)
```

下面是随机生成数据，从$[-\pi,\pi]$上随机选取了 10， 50， 200个点作为训练数据，随机选取了1000个点作为测试数据。

```python
X_train = np.linspace(-np.pi, np.pi, 10)
X_test = np.linspace(-np.pi, np.pi, 1000)
Y_train = np.sin(X_train)
Y_test = np.sin(X_test)
```

实例化网络，这里选择三层的网络，只有一个隐藏层，隐藏层含有10个神经元。

```python
model = Network([1, 10, 1])
```

调用反向传播算法进行训练，完成回归拟合。这里学习率0.01，次数选择了3000次。

```python
model.train(X_train, Y_train, learning_rate=0.01, epochs=3000)
```

计算测试误差，绘制训练数据和最终拟合的曲线。

```python
result = [float(model.forward(i)) for i in X_test]
result = np.array(result)
print(f"average error: {np.mean(Y_test-result)}")


plt.plot(X_train, Y_train, label='sin(x)')
plt.plot(X_test, result, label='fitting curve')
plt.legend()
plt.show()
```

下面是10，50， 1000个点作为训练数据所得结果图。`average error`为实际值与预测值的差的绝对值的期望。其中1000个点的网络采用了`[1, 20, 20, 1]`的网络架构，以满足0.01的误差。

![image-20230410201507519](/Users/xieyiweng/Library/Application Support/typora-user-images/image-20230410201507519.png)

![image-20230410201539742](/Users/xieyiweng/Library/Application Support/typora-user-images/image-20230410201539742.png)

![image-20230410204217612](/Users/xieyiweng/Library/Application Support/typora-user-images/image-20230410204217612.png)



#### 任务2: 对12个汉字进行分类

对12个汉字进行分类，需要进行以下步骤：图片预处理，设计网络结构，训练和测试，可视化。

首先构建了图片的预处理类`Preprocess`，完成图片和标签的读入和预处理，将数据集裁成两份已完成训练和评估。

`__init__`初始化构造函数通过文件路径，读入了图像的路径和标签，并将其分别保存在`X`和`Y`两个列表中。这里对`Y`中的标签进行`one-shot`处理。将`X`和`Y`转化为np的数组，方便处理。

`split`为切割训练集的函数，调用了`sklearn.model_selection`，实现了按照自己的比例随机划分训练集合，这里同时对划分后的集合进行了**图片读取**，**归一化**的操作。

```python
class Preprocess:
    def __init__(self, path):
        X = []  # 定义图像名称
        Y = []  # 定义图像标签
        for i in range(1, 13):
            imgList = os.listdir(path + "/%s" % i)
            imgList.sort(key=lambda x: int(x.split('.')[0]))  # 按顺序读如图片
            for name in imgList:
                X.append(path + "/" + str(i) + "/" + str(name))
                vector = np.zeros(12)
                vector[i - 1] = 1
                Y.append(vector)
        self.X = np.array(X)
        self.Y = np.array(Y)

    def split(self, test_size: float):
        X_train, X_test, Y_train, Y_test = train_test_split(self.X, self.Y, test_size=test_size, random_state=2)
        XX_train = [cv2.imread(i, cv2.IMREAD_GRAYSCALE) for i in X_train]
        XX_test = [cv2.imread(i, cv2.IMREAD_GRAYSCALE) for i in X_test]
        XX_train = np.array(XX_train) / 255.0
        XX_test = np.array(XX_test) / 255.0
        return XX_train, XX_test, Y_train, Y_test
      
   	def testread(self):
        X_test = [cv2.imread(i, cv2.IMREAD_GRAYSCALE) for i in self.X]
        X_test = np.array(X_test) / 255.0
        return X_test, self.Y
```

下面的代码是网络结构，与上一部分的代码有一定的相似度，故将重点解释差异部分。

`__init__`构造函数与之前一致。

`sigmod`和`dsigmod`与之前一致，但是后来发现`relu`函数训练更快，故更换。

`luckyrelu`和`dluckyrelu`是激活函数和其导数，这里优化了`relu`函数为`leakyrelu`，为了避免神经元死亡。

`forward`为前向计算与之前一致。

`backward`为逆向计算，这里添加了一点代码以计算准确率。

`train`为训练函数，为了实时画图可视化添加了一些代码，展现了在自行划分的训练集和测试集上的`accuracy`，期望出现过拟合现象，但是由于数据量过小所以没能实现。同时将训练获得的参数保存下来，以便测试使用。

`predict_for_train`和`predict`都是预测函数，差别是一个是用现有参数预测，一个是用保存的参数预测，分别对应训练时的实时可视化和测试时的预测。二者都实现了`accuracy`的输出。

```python
class Network:
    def __init__(self, layers):
        self.layers = layers
        self.weights = []
        self.biases = []
        for i in range(1, len(layers)):
            self.weights.append(np.random.randn(layers[i], layers[i - 1]) / np.sqrt(layers[i - 1]))
            self.biases.append(np.random.randn(layers[i], 1) / np.sqrt(layers[i]))

    def sigmod(self, x):
        return 1 / (1 + np.exp(-x))

    def dsigmod(self, x):
        return self.sigmod(x) * (1 - self.sigmod(x))

    def leakyrelu(self, x):
        return np.maximum(0.01 * x, x)

    def dleakyrelu(self, x):
        dx = np.ones_like(x)
        dx[x < 0] = 0.01
        return dx

    def forward(self, a):
        for w, b in zip(self.weights, self.biases):
            z = np.dot(w, a) + b
            a = self.leakyrelu(z)
        return a

    def backward(self, x, y, learning_rate, accuracy):
        a_s = [x]
        z_s = []
        for i in range(len(self.layers) - 1):
            z = np.dot(self.weights[i], a_s[-1]) + self.biases[i]
            z_s.append(z)
            a = self.leakyrelu(z)
            a_s.append(a)
        if np.argmax(a_s[-1]) == np.argmax(y):
            accuracy[0] += 1
        delta = (a_s[-1] - y) * self.dleakyrelu(z_s[-1])
        for i in range(len(self.layers) - 2, -1, -1):
            dw = np.dot(delta, a_s[i].T)
            db = delta
            if i != 0:  # 坑死爹了
                delta = np.dot(self.weights[i].T, delta) * self.dleakyrelu(z_s[i - 1])  # 是z，是i （坑死老子了）
            self.weights[i] -= learning_rate * dw
            self.biases[i] -= learning_rate * db

    def train(self, X, Y, epochs, learning_rate=0.01, draw='no'):

        fig, ax = plt.subplots()
        plt.ion()
        x_data = []
        y1_data = []
        y2_data = []
        for epoch in range(epochs):
            print(f"epoch={epoch + 1} is running")
            accuracy = [0]
            for i in range(len(X)):
                x = X[i].reshape(-1, 1)
                y = Y[i].reshape(-1, 1)
                self.backward(x, y, learning_rate, accuracy)

            # 画画
            if draw != 'no':
                x_data.append(epoch)
                y1_data.append(self.predict_for_train(X_test, Y_test))
                y2_data.append(accuracy[0] / len(X))
                ax.clear()
                ax.plot(x_data, y1_data, y2_data)
                plt.title("Accuracy Curve")
                plt.xlabel("Epoch")
                plt.ylabel("Accuracy")
                plt.pause(0.01)

            if epoch == 30:
                with open('model_weights5.pkl', 'wb') as f:
                    pickle.dump(self.weights, f)
                with open('model_biases5.pkl', 'wb') as f:
                    pickle.dump(self.biases, f)

    def predict_for_train(self, X, Y):
        accuracy = 0
        for i, j in zip(X, Y):
            prediction = self.forward(i.reshape(-1, 1))
            result = np.argmax(prediction) + 1
            actual = np.argmax(j) + 1
            if result == actual:
                accuracy += 1
            # print(f"Prediction: {result},  Actual：{actual}")
        print(f"Accuracy: {accuracy / len(Y)}")
        return accuracy / len(Y)

    def predict(self, X, Y):
        with open('model_weights3.pkl', 'rb') as f:
            self.weights = pickle.load(f)
        with open('model_biases3.pkl', 'rb') as f:
            self.biases = pickle.load(f)
        accuracy = 0
        for i, j in zip(X, Y):
            prediction = self.forward(i.reshape(-1, 1))
            result = np.argmax(prediction) + 1
            actual = np.argmax(j) + 1
            if result == actual:
                accuracy += 1
            # print(f"Prediction: {result},  Actual：{actual}")
        print(f"Accuracy: {accuracy / len(Y)}")
        return accuracy / len(Y)
```

最后是`mian`函数完成训练和测试。

```python
if __name__ == "__main__":
    preprocess = Preprocess('./train')
    model = Network([784, 128, 12])
    # for train
    X_train, X_test, Y_train, Y_test = preprocess.split(0.3)
    model.train(X_train, Y_train, learning_rate=0.01, epochs=100, draw="yes")

    # for test
    X_test, Y_test = preprocess.testread()
    model.predict(X_test, Y_test)
```

**结果展示：**

下图为`[784, 128, 10]`	`learning_rate` = 0.01

![image-20230410215147171](/Users/xieyiweng/Library/Application Support/typora-user-images/image-20230410215147171.png)

下图为：`[784, 256, 128, 10]`	`learning_rate` = 0.005

![image-20230410215736181](/Users/xieyiweng/Library/Application Support/typora-user-images/image-20230410215736181.png)



#### 总结

对于part1的两个任务，设计了非常相似的bp神经网络结构，实现了对$sin(x)$的函数拟合和12个汉字的图像分类。

对于两个任务都尝试了不同的神经网络层数和神经元数目，调整了学习率和训练次数，均获得不同的训练效果。最终更好的结果需要多次尝试才能得出。根据任务的不同特点使用了不同的激活函数，其对结果和计算素的均有影响。

两个实验都进行了可视化，其中分类任务由于数据量过低无法看到过拟合现象。