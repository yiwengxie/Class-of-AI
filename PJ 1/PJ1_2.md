## 使用卷积神经网络实现12个手写汉字分类

#### 代码结构以及理解

本代码采用了`torch`库来完成神经网络的实现。定义了一个`CnnNet`的网络，该网络继承了`nn.Module`，包含`__init__`初始化构造函数，`forward`前向函数，`predict`预测函数，`train`训练函数。

`__init__`函数首先调用父函数进行初始化，以使用父函数中定义的神经网络层和函数。`self.conv1`是二维卷积层，输入通道为1，输出通道为6，卷积核大小为5，图片维度$28\times 28 \rightarrow 18\times18$。`self.pool`是最大池化层，池化核大小为2，步长为2，图片维度$18\times 18 \rightarrow 8\times8$。`self.con2`是二维卷积层，输入通道为6，输出通道为16，卷积核大小为5，图片维度$8\times8 \rightarrow 4\times4$。`self.fc1`是全连接层，输入节点个数为16个通道乘上4*4的图片大小即1644，输出通道为120。`self.fc2` 是全连接层，输入节点120个，输出节点84个。`self.fc3`为全连接层，输入节点84个，输出节点12个。`self.dropout`是dropout层，随机丢弃概率为0.5，使得网络不依赖任何单个神经元进行预测，提高鲁棒性。

`forward`函数展现了网络的前向结构。先是卷积层，采用`relu`激活函数，再进入池化层，随后再是卷积层，`relu`激活函数，池化层。调用`view`函数将卷积层输出展平为二维张量。接下来是两个全连接层，每个都用`relu`作为激活函数，每层后面采用`dropout`，提高鲁棒性。最后一层为全连接层没有采用`softmax`，是因为后续的`CrossEntropyLoss`中包含了softmax函数。

`train`函数为网络训练函数。这里定义了损失，当训练集上的损失连续五次没有下降则提前停止训练，当验证集准确率到达95.0也提前停止训练。对每个`epoch`，将训练集分成[4]个一组的小批次，梯度清零，计算输出，采用`CrossEntropyLoss`计算损失，反向传播，更新参数。在验证集上验证准确率，若达到95.0则提前停止，若准确率提高则保存参数。

`predict`函数为预测函数。`with torch.no_grad()`不需要反向传播，关闭梯度计算。对小批次进行迭代，计算预测值，计算准确率。

```python
class CnnNet(nn.Module):
    def __init__(self) -> None:
        super(CnnNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 4 * 4, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 12)
        self.dropout = nn.Dropout(p=0.5)

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        x = self.dropout(x) # 全连接层中间的dropout层
        x = torch.relu(self.fc2(x))
        x = self.dropout(x) # 全连接层中间的dropout层
        x = self.fc3(x)
        return x
    
    def train(self):
        best_acc = 0
        loss_threshold = 5 # 连续5次训练集损失没有下降就停止训练
        prev_loss = float('inf')
        no_improvement_count = 0

        for epoch in range(30):
            running_loss = 0.0
            for i, data in enumerate(train_loader, 0):
                inputs, labels = data
                inputs, labels = inputs.to(device), labels.to(device)

                optimizer.zero_grad()
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()

                running_loss += loss.item()
                if i % 100 == 99:
                    print('[%d, %5d] loss: %.3f' % (epoch+1, i+1, running_loss/100))
                    if running_loss >= prev_loss:
                        no_improvement_count += 1
                    else:
                        no_improvement_count = 0
                    prev_loss = running_loss
                    running_loss = 0.0
            
            if no_improvement_count >= loss_threshold:
                print('Training stopped early due to no improvement in %d epochs.' % loss_threshold)
                return
            
            correct = 0
            total = 0
            with torch.no_grad():
                for data in test_loader:
                    images, labels = data
                    images, labels = images.to(device), labels.to(device)

                    outputs = model(images)
                    _, predicted = torch.max(outputs.data, 1)
                    total += labels.size(0)
                    correct += (predicted == labels).sum().item()
            acc = 100 * correct / total
            print('Accuracy on the test set: %d %%' % acc)

            if acc > 95.0:
                print('Accuracy reached the threshold, training stopped.')
                return
            if acc > best_acc:
                best_acc = acc
                torch.save(model.state_dict(), './model_params.pt')

    def predict(self):
        correct = 0
        total = 0
        with torch.no_grad():
            for data in test_loader:
                images, labels = data
                images, labels = images.to(device), labels.to(device)

                outputs = model(images)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        print('Accuracy on the test set: %d %%' % (100 * correct / total)) 
```

`transform`定义了图像的处理，这里将图像转换为灰度图像，`array`转换为`tensor`，每个通道进行标准化，均值为0.5，标准差为0.5，防止梯度爆炸或梯度消失。

```python
transform = transforms.Compose(
    [transforms.Grayscale(),
     transforms.ToTensor(),
     transforms.Normalize((0.5), (0.5))]
)
```

这里读取训练集和测试集，这里的训练集和测试集是我自己划分的。`batch_size`选择了4，训练集需要每次打乱顺序。

```python
trainset = torchvision.datasets.ImageFolder(root = './train2', transform=transform)
train_loader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)

testset = torchvision.datasets.ImageFolder(root='./test2', transform=transform)
test_loader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False)
```

`device`是看用gpu还是cpu跑，`mode`l调用了`CnnNet`，`criterion`采用了交叉熵损失函数，`opimizer`用了Adam优化器，采用了L2正则化。

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CnnNet().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001) # L2正则化系数
```

#### Bonus

采用了`L2正则化系数`防止过拟合，L2 正则化通过在损失函数中加入正则化项来限制模型参数的大小，从而使模型参数更加稳定和平滑。

选择了提前结束模型训练，结束条件为连续五次`loss`不下降，或者在验证集上达到了理想精度。

采用了`dropout`来提升模型的鲁棒性。

![image-20230418123934551](/Users/xieyiweng/Library/Application Support/typora-user-images/image-20230418123934551.png)